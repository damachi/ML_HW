{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from implementations import *\n",
    "from proj1_helpers import *\n",
    "import matplotlib.pyplot as plt\n",
    "from costs import *\n",
    "from hw_helpers import *\n",
    "from cross_validation import *\n",
    "from visualization import *\n",
    "from gradients import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    #print(np.nanmean(x, axis=0))\n",
    "    centered_data = np.subtract(x, np.nanmean(x, axis=0))\n",
    "    std_data = centered_data / np.nanstd(centered_data, axis=0)\n",
    "    return std_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize_test(x_tr, x_te):\n",
    "    mean = np.nanmean(x_tr, axis=0)\n",
    "    centered_data = np.subtract(x_te, mean)\n",
    "    std = np.nanstd(x_tr, axis=0)\n",
    "    std_data = centered_data/std\n",
    "    return std_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    loss = (np.log(1 + np.exp(tx@w))- (y*tx@w) - lambda_/2*(np.linalg.norm(w)*np.linalg.norm(w))).sum()\n",
    "    gradient = tx.T@(sigmoid(tx@w)-y) - lambda_*w\n",
    "    #print(\"gradient\", gradient)\n",
    "    return loss, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "    loss, gradient = penalized_logistic_regression(y,tx, w, lambda_)\n",
    "    #print(\"w\", w)\n",
    "    #print(\"gg\", gradient*gamma)\n",
    "    w = w-(gradient*gamma)\n",
    "    return loss,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_penalized_gradient_descent_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    gamma = 0.000000001\n",
    "    lambda_ = 100\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    w = np.zeros((x.shape[1],1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for it in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y, x, w, gamma, lambda_)\n",
    "        #print(loss.shape, w.shape)\n",
    "        # log info\n",
    "        if it % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=it, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "    #visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_penalized_gradient_descent\")\n",
    "    #print(\"loss={l}\".format(l=calculate_loss(y, x, w)))\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yb, input_data, ids = load_csv_data('Data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yb_te, input_data_te, ids_te = load_csv_data('Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nan in the last colums are represented by 0\n",
    "#np.place(input_data[:,29], input_data[:,29] == 0, np.nan)\n",
    "input_data = np.delete(input_data, [4,5,6,12,23,26,27,28,29],axis= 1)\n",
    "#np.place(input_data_te[:,29], input_data_te[:,29] == 0, np.nan)\n",
    "input_data_te = np.delete(input_data_te, [4,5,6,12,23,26,27,28,29],axis= 1)\n",
    "np.place(input_data, input_data < -998, np.nan)\n",
    "np.place(input_data_te, input_data_te < -999, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#standarization\n",
    "#std_input = standardize(input_data)\n",
    "#std_input_te = standardize(input_data_te)\n",
    "#std_input = np.nan_to_num(std_input)\n",
    "#std_input_te = np.nan_to_num(std_input_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yb = yb.reshape(yb.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yb_te = yb_te.reshape(yb_te.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yb_tr, tx_tr = build_model_data(yb, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_split = np.hsplit(tx_tr,tx_tr.shape[1])\n",
    "degree = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_tr_tx = h_split[0]\n",
    "for x in h_split[1:]:\n",
    "    poly_tr_tx = np.hstack((poly_tr_tx, build_poly(x.ravel(),3)[:,1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "poly_tr_tx = standardize(poly_tr_tx)\n",
    "#std_input_te = standardize(input_data_te)\n",
    "poly_tr_tx = np.nan_to_num(poly_tr_tx)\n",
    "#std_input_te = np.nan_to_num(std_input_te)\n",
    "poly_train = poly_tr_tx[:200000]\n",
    "poly_test = poly_tr_tx[200000:]\n",
    "y_train = yb[:200000]\n",
    "y_test = yb[200000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00028297685549754444"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_train[:,1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=138629.43611198905\n",
      "Current iteration=100, loss=134681.9674584266\n",
      "Current iteration=200, loss=127028.77705838735\n",
      "Current iteration=300, loss=115893.85666799589\n",
      "Current iteration=400, loss=101483.90832085184\n",
      "Current iteration=500, loss=83989.37622781296\n",
      "Current iteration=600, loss=63585.634478318774\n",
      "Current iteration=700, loss=40434.20962711259\n",
      "Current iteration=800, loss=14683.97663556238\n",
      "Current iteration=900, loss=-13527.708454571946\n",
      "Current iteration=1000, loss=-44073.95789586362\n",
      "Current iteration=1100, loss=-76837.39048431875\n",
      "Current iteration=1200, loss=-111709.25843760002\n",
      "Current iteration=1300, loss=-148588.65881401207\n",
      "Current iteration=1400, loss=-187381.8234839504\n",
      "Current iteration=1500, loss=-228001.48090169736\n",
      "Current iteration=1600, loss=-270366.28281526145\n",
      "Current iteration=1700, loss=-314400.28930628486\n",
      "Current iteration=1800, loss=-360032.5059961049\n",
      "Current iteration=1900, loss=-407196.4677822988\n",
      "Current iteration=2000, loss=-455829.8640198242\n",
      "Current iteration=2100, loss=-505874.20059630927\n",
      "Current iteration=2200, loss=-557274.4948529218\n",
      "Current iteration=2300, loss=-609978.9997616019\n",
      "Current iteration=2400, loss=-663938.9541833884\n",
      "Current iteration=2500, loss=-719108.3564018416\n",
      "Current iteration=2600, loss=-775443.7584527264\n",
      "Current iteration=2700, loss=-832904.0790597212\n",
      "Current iteration=2800, loss=-891450.433239795\n",
      "Current iteration=2900, loss=-951045.9768648678\n",
      "Current iteration=3000, loss=-1011655.7646620821\n",
      "Current iteration=3100, loss=-1073246.620306736\n",
      "Current iteration=3200, loss=-1135787.0174126988\n",
      "Current iteration=3300, loss=-1199246.9703575692\n",
      "Current iteration=3400, loss=-1263597.9339962532\n",
      "Current iteration=3500, loss=-1328812.7114191435\n",
      "Current iteration=3600, loss=-1394865.3690013536\n",
      "Current iteration=3700, loss=-1461731.1580691854\n",
      "Current iteration=3800, loss=-1529386.4425803607\n",
      "Current iteration=3900, loss=-1597808.6322768347\n",
      "Current iteration=4000, loss=-1666976.1208242\n",
      "Current iteration=4100, loss=-1736868.2285006063\n",
      "Current iteration=4200, loss=-1807465.1490416164\n",
      "Current iteration=4300, loss=-1878747.9002861113\n",
      "Current iteration=4400, loss=-1950698.2783027878\n",
      "Current iteration=4500, loss=-2023298.814707587\n",
      "Current iteration=4600, loss=-2096532.736909773\n",
      "Current iteration=4700, loss=-2170383.9310490354\n",
      "Current iteration=4800, loss=-2244836.9074079157\n",
      "Current iteration=4900, loss=-2319876.7681036163\n",
      "Current iteration=5000, loss=-2395489.176880996\n",
      "Current iteration=5100, loss=-2471660.330844421\n",
      "Current iteration=5200, loss=-2548376.9339805236\n",
      "Current iteration=5300, loss=-2625626.172336861\n",
      "Current iteration=5400, loss=-2703395.690733073\n",
      "Current iteration=5500, loss=-2781673.570891713\n",
      "Current iteration=5600, loss=-2860448.310885451\n",
      "Current iteration=5700, loss=-2939708.8058059746\n",
      "Current iteration=5800, loss=-3019444.329567715\n",
      "Current iteration=5900, loss=-3099644.517766656\n",
      "Current iteration=6000, loss=-3180299.3515209095\n",
      "Current iteration=6100, loss=-3261399.1422256\n",
      "Current iteration=6200, loss=-3342934.5171599807\n",
      "Current iteration=6300, loss=-3424896.405889459\n",
      "Current iteration=6400, loss=-3507276.0274098297\n",
      "Current iteration=6500, loss=-3590064.8779848493\n",
      "Current iteration=6600, loss=-3673254.7196321576\n",
      "Current iteration=6700, loss=-3756837.5692158295\n",
      "Current iteration=6800, loss=-3840805.688107018\n",
      "Current iteration=6900, loss=-3925151.5723769316\n",
      "Current iteration=7000, loss=-4009867.94348898\n",
      "Current iteration=7100, loss=-4094947.7394594476\n",
      "Current iteration=7200, loss=-4180384.106458008\n",
      "Current iteration=7300, loss=-4266170.390821679\n",
      "Current iteration=7400, loss=-4352300.131457509\n",
      "Current iteration=7500, loss=-4438767.05261098\n",
      "Current iteration=7600, loss=-4525565.056978856\n",
      "Current iteration=7700, loss=-4612688.219146466\n",
      "Current iteration=7800, loss=-4700130.779330888\n",
      "Current iteration=7900, loss=-4787887.137412685\n",
      "Current iteration=8000, loss=-4875951.847239977\n",
      "Current iteration=8100, loss=-4964319.611189757\n",
      "Current iteration=8200, loss=-5052985.274972267\n",
      "Current iteration=8300, loss=-5141943.822665239\n",
      "Current iteration=8400, loss=-5231190.37196554\n",
      "Current iteration=8500, loss=-5320720.169646643\n",
      "Current iteration=8600, loss=-5410528.5872111\n",
      "Current iteration=8700, loss=-5500611.116727674\n",
      "Current iteration=8800, loss=-5590963.366843649\n",
      "Current iteration=8900, loss=-5681581.058963296\n",
      "Current iteration=9000, loss=-5772460.023584017\n",
      "Current iteration=9100, loss=-5863596.196782213\n",
      "Current iteration=9200, loss=-5954985.616841464\n",
      "Current iteration=9300, loss=-6046624.421015858\n",
      "Current iteration=9400, loss=-6138508.842421955\n",
      "Current iteration=9500, loss=-6230635.207053096\n",
      "Current iteration=9600, loss=-6322999.930910136\n",
      "Current iteration=9700, loss=-6415599.517243087\n",
      "Current iteration=9800, loss=-6508430.55389845\n",
      "Current iteration=9900, loss=-6601489.710767184\n",
      "-6693839.79444\n"
     ]
    }
   ],
   "source": [
    "e, w = logistic_regression_penalized_gradient_descent_demo(y_train,poly_train)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n",
      "/Users/lucasgauchoux/Documents/MachineLearning/MLrepo/ML_HW/hw_helpers.py:31: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1 + np.exp(-t))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(45, 1)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, gradient = penalized_logistic_regression(poly_train@w,poly_train,w,0)\n",
    "\n",
    "gradient.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = predict_labels(w, poly_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "false_pred = 0\n",
    "for i in range(len(test_data)):\n",
    "    if not test_data[i]==y_test[i]:\n",
    "        false_pred += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13826"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.4579309133041534e-15"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_tr_tx[:,3].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yb_te, tx_te = build_model_data(yb_te, std_input_te)\n",
    "h_split = np.hsplit(tx_te,tx_te.shape[1])\n",
    "degree = 3\n",
    "poly_tr_te = h_split[0]\n",
    "for x in h_split[1:]:\n",
    "    poly_tr_te = np.hstack((poly_tr_te, build_poly(x.ravel(),3)[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = predict_labels(w, poly_tr_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_csv_submission(ids_te, y_pred, 'ridge_poly_constant.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
